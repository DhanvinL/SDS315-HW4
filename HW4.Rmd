---
title: "HW4"
author: "Dhanvin Lakshmisha"
date: '`r Sys.Date()`'
output:
  html_document:
    df_print: paged
---

Dhanvin Lakshmisha

dl37833

SDS 315

GitHub link - https://github.com/DhanvinL/SDS315-HW4

```{r message = FALSE, warning = FALSE, echo = FALSE}
library(ggplot2)
library(tidyverse)
library(dplyr)
library(knitr)
library(mosaic)

letter_frequencies <- read.csv("letter_frequencies.csv")


```


## Question 1

The null hypothesis is that the probability of security trades getting flagged from Iron Bank is 2.4%, which is the same rate as that of other traders.

The test statistic is the number of flagged trades, which is 70 in this case.

Here is a plot of the probability of the test statistic assuming the null hypothesis is true. 

```{r message = FALSE, warning = FALSE, echo = FALSE}
sim_flags <- do(100000) * nflip(n = 2021, prob = 0.024)

ggplot(sim_flags) +
  geom_histogram(aes(x = nflip), binwidth = 1) +
  labs(title = "Distribution of Flagged Trades",
       x = "Number of Flagged Trades",
       y = "Count") 



```


Based on our test statistic, we can find the p-value to see how our null hypothesis holds up. 

```{r message = FALSE, warning = FALSE, echo = FALSE}
p_value <- sum(sim_flags$nflip >= 70) /100000

p_value

```

The P-value which is less than .05 suggests that the Iron Bank's trading activity is inconsistent with the expected rate of other traders.


## Question 2

The null hypothesis is that the probability of restaurants in the city getting a health code violation is 3%, which is the city average.

The test statistic is the number of violations in 50 inspections, which in this case is 8.

Here is the probability distribution for the statistic under the assumption the null hypothesis is true.


```{r message = FALSE, warning = FALSE, echo = FALSE}
sim_violations <- do(100000) * nflip(n = 50, prob = .03)

ggplot(sim_violations) +
  geom_histogram(aes(x = nflip), binwidth = 1) +
  labs(title = "Distribution of Health Code Violations",
       x = "Number of Violations in 50 Inspections",
       y = "Count")

```



Based on this distribution, we can find the p-value. 


```{r message = FALSE, warning = FALSE, echo = FALSE}
p_value <- sum(sim_violations$nflip >= 8) / 100000
p_value
```

The very low p-value tells us that the Gourmet Bites' violation rate is significantly higher than the expected citywide violation rate. 


## Question 3


The null hypothesis is that the distribution of jurors listed by the judge is not significantly different from the country's population proportions. 

The test statistic is the chi square test statistic. For this instance, the statistic is - 

```{r message = FALSE, warning = FALSE, echo = FALSE}
observed_counts <- c(85, 56, 59, 27, 13)
expected_probs <- c(0.30, 0.25, 0.20, 0.15, 0.10)
expected_counts <- expected_probs * (20*12)
chi_square_stat <- sum((observed_counts - expected_counts)^2 / expected_counts)
chi_square_stat


```

To understand this statistic we have to create a chi-square distribution of it. We are simulating the distribution through multinomial sampling.

```{r message = FALSE, warning = FALSE, echo = FALSE}
sim_chi_square <- do(100000) * {
  simulated_counts <- rmultinom(1, size = (20*12), prob = expected_probs)
  sum((simulated_counts - expected_counts)^2 / expected_counts)
}

ggplot(sim_chi_square) +
  geom_histogram(aes(x = result), binwidth = 1) +
  labs(title = "Distribution of Chi-Square Statistic",
       x = "Chi-Square Statistic",
       y = "Count") 


```

Based on this distribution we can get a p-value. 


```{r message = FALSE, warning = FALSE, echo = FALSE}
p_value <- sum(sim_chi_square$result >= chi_square_stat) / 100000
p_value


```

The p-value is `r p_value`


The low p-value suggests that the distribution of jurors from this judge signiicantly differs from the expected distribution of judges. 


## Question 4

### Part A

To see which sentence is watermarked by an LLM, we can create a null distribution or reference. This is based on the the chi-squared values of each sentence. 


```{r message = FALSE, warning = FALSE, echo = FALSE}

#STEP 1 and 2
brown_sentences <- readLines("brown_sentences.txt")
clean_sentences <- gsub("[^A-Za-z]","", brown_sentences)
clean_sentences <- toupper(clean_sentences)

#STEP 3
letter_distributions <- matrix(0, nrow = length(clean_sentences), ncol = 26)
colnames(letter_distributions) <- LETTERS




calculate_letter_distribution <- function(sentence) {
  letter_counts <- table(factor(strsplit(sentence, "")[[1]], levels = LETTERS))
  return(as.numeric(letter_counts))
}

for (i in 1:length(clean_sentences)) {
  letter_distributions[i, ] <- calculate_letter_distribution(clean_sentences[i])
}




```

```{r message = FALSE, warning = FALSE, echo = FALSE}

#STEP 4 and 5


calculate_chi_square <- function(observed_counts, total_letters, expected_probs) {
  expected_counts <- total_letters * expected_probs 
  chi_sq <- sum((observed_counts - expected_counts)^2 / expected_counts, na.rm = TRUE)
  return(chi_sq)
}

total_letters_per_sentence <- rowSums(letter_distributions)  
chi_squared_values <- numeric(length(clean_sentences)) 


for (i in 1:length(clean_sentences)) {
  chi_squared_values[i] <- calculate_chi_square(
    observed_counts = letter_distributions[i, ],
    total_letters = total_letters_per_sentence[i],
    expected_probs = letter_frequencies$Probability
  )
}

chi_sq_df <- data.frame(chi_squared_values)

#STEP 6
ggplot(chi_sq_df, aes(x = chi_squared_values)) +
  geom_histogram(binwidth = 2) +
  labs(title = "Distribution of Letter Frequencies in Normal Sentences",
       x = "Chi-Squared Value",
       y = "Count") 


```


### Part B

Now, we can output a table of p-values for each sentence given. 

```{r message = FALSE, warning = FALSE, echo = FALSE}
sentences <- c(
  "She opened the book and started to read the first chapter, eagerly anticipating what might come next.",
  "Despite the heavy rain, they decided to go for a long walk in the park, crossing the main avenue by the fountain in the center.",
  "The museum's new exhibit features ancient artifacts from various civilizations around the world.",
  "He carefully examined the document, looking for any clues that might help solve the mystery.",
  "The students gathered in the auditorium to listen to the guest speaker's inspiring lecture.",
  "Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland.",
  "The chef demonstrated how to prepare a delicious meal using only locally sourced ingredients, focusing mainly on some excellent dinner recipes from Spain.",
  "They watched the sunset from the hilltop, marveling at the beautiful array of colors in the sky.",
  "The committee reviewed the proposal and provided many points of useful feedback to improve the project's effectiveness.",
  "Despite the challenges faced during the project, the team worked tirelessly to ensure its successful completion, resulting in a product that exceeded everyone's expectations."
)






calculate_chi_square <- function(observed_counts, total_letters, expected_probs) {
  expected_counts <- total_letters * expected_probs 
  chi_sq <- sum((observed_counts - expected_counts)^2 / expected_counts)
  return(chi_sq)
}

clean_test_sentences <- gsub("[^A-Za-z]","", sentences)
clean_test_sentences <- toupper(sentences)


test_chi_squared <- numeric(length(sentences))
for (i in 1:length(clean_test_sentences)) {
  observed_counts <- calculate_letter_distribution(clean_test_sentences[i])
  total_letters <- sum(observed_counts)
  test_chi_squared[i] <- calculate_chi_square(
    observed_counts = observed_counts,
    total_letters = total_letters,
    expected_probs = letter_frequencies$Probability
  )
}

p_values <- numeric(length(sentences))
for (i in 1:length(sentences)) {
p_values[i] <- sum(chi_squared_values >= test_chi_squared[i]) / length(chi_squared_values)}

p_value_table <- data.frame(
  Sentence = 1:10,
  P_Value = round(p_values, 3)
)

p_value_table

```


Based on the distribution, we can assume that sentence 6 is the sentence produced by an LLM. This is due to it having the lowest p-value relative to other sentences. Also, it is the only p-value less than .05, which is the chosen significance level. If our null hypothesis was that each sentence follows the typical English letter frequency distribution (no watermark), we would reject the null hypothesis for sentence 6. Sentence 6's letter frequencies are significantly different from that of English text.









